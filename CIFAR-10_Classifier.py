# -*- coding: utf-8 -*-
"""CS178_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iO1q0f5AMvw9cxBNdvefq3EOTF7d6Fh6
"""

from keras.datasets import cifar10
import matplotlib.pyplot as plt
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

seed = 1234
np.random.seed(seed)

"""Source link(CIFAR-10): https://www.cs.toronto.edu/~kriz/cifar.html

"""

(X_train, y_train), (X_test, y_test) = cifar10.load_data() #loading cifar-10 data
classes = np.array(['plane', 'car', 'bird', 'cat', 'deer',
                    'dog', 'frog', 'horse', 'ship', 'truck']) #classes in cifar-10

y_train = y_train.reshape((-1,))
y_test = y_test.reshape((-1, ))

y_train.shape, y_test.shape

# original_X_train, original_y_train = X_train, y_train
# # create train validation data (80/20)
# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8, random_state=seed)
# original_X_train.shape, original_y_train.shape, X_train.shape, X_val.shape, y_train.shape, y_val.shape

print(X_train.shape)

print(X_test.shape)
print(y_test[:10])

print(y_train.shape)
print(y_test.shape)

print(X_test[:,0,0,0])

"""[link text](https://)"""

print(classes[y_train[7]]) #class is represented by numbers, use classes[index] to cast if needed
plt.imshow(X_train[7])
plt.show()

fig, axes = plt.subplots(10, 5, figsize=(10, 20))

for i in range(10):
    images_w_label = X_train[y_train == i][:5]

    axes[i,0].set_ylabel(classes[i])
    for j in range(5):
      axes[i,j].imshow(images_w_label[j], aspect="auto")
      axes[i,j].tick_params(left = False, bottom=False, labelleft=False, labelbottom=False)

plt.tight_layout()
# plt.subplots_adjust(hspace=0, wspace=0)

"""# KNN Classifier"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
#transform (32, 32, 3) into (3072, )
print(X_train.shape)
knn_X_train = X_train.reshape(-1,3072)
# knn_X_val = X_val.reshape(-1, 3072)
knn_X_test = X_test.reshape(-1, 3072)
knn_X_train.shape, knn_X_test.shape

np.geomspace(1, 80, 10, dtype=int)

def plot_complexity_error(values, error_train, error_test):
  fig, axes = plt.subplots()
  axes.semilogx(values, error_train, label="training error")
  axes.semilogx(values, error_test, label="testing error")

  axes.set_xlabel("k")
  axes.set_ylabel("Error")
  axes.legend()

"""## Hyperparameter tuning"""

sample_size = 1000
k_values = np.geomspace(1, 80, 10, dtype=int)

knn = KNeighborsClassifier()

gs_knn = GridSearchCV(knn, {"n_neighbors": k_values}, return_train_score=True)

gs_knn.fit(knn_X_train[:sample_size], y_train[:sample_size])

error_test = 1 - gs_knn.cv_results_['mean_test_score']
error_train = 1 - gs_knn.cv_results_['mean_train_score']

plot_complexity_error(k_values, error_train, error_test)
print('Best parameters found:\n',gs_knn.best_params_)

errors_train = []
errors_test = []

train_size, val_size = 10000, 2000
print(train_size, val_size)

for k in np.geomspace(1, 80, 10, dtype=int):
  knn = KNeighborsClassifier(n_neighbors=k)
  knn.fit(knn_X_train[:train_size], y_train[:train_size])

  errors_train.append(1 - knn.score(knn_X_train[:train_size], y_train[:train_size]))
  errors_test.append(1- knn.score(knn_X_val[:val_size], y_val[:val_size]))

plot_complexity_error(k_values, error_train, error_test)
print('Best parameters found:\n',gs_knn.best_params_)

def knn_classifier_cifar(knn_X_train, knn_y_train, knn_X_test, knn_y_test):
  knn = KNeighborsClassifier(n_neighbors=11)
  print(knn_X_train.shape,knn_X_test.shape)
  knn.fit(knn_X_train, knn_y_train)

  knn_y_train_pred = knn.predict(knn_X_train)
  knn_y_test_pred = knn.predict(knn_X_test)

  train_acc = accuracy_score(knn_y_train, knn_y_train_pred)
  test_acc = accuracy_score(knn_y_test, knn_y_test_pred)
  print(f'Train Accuracy: {train_acc * 100:.3f}%')
  print(f'Test Accuracy: {test_acc * 100:.3f}%')
  return knn, knn_y_train_pred, knn_y_test_pred

knn = KNeighborsClassifier(n_neighbors=11)
test_size, training_size = 1000, 20000
print(knn_X_train.shape,knn_X_test.shape)
knn.fit(knn_X_train[:training_size], y_train[:training_size])

knn_y_train_pred = knn.predict(knn_X_train[:training_size])
knn_y_test_pred = knn.predict(knn_X_test[:test_size])

train_acc = accuracy_score(y_train[:training_size], knn_y_train_pred[:training_size])
test_acc = accuracy_score(y_test[:test_size], knn_y_test_pred[:test_size])
print(f'Train Accuracy: {train_acc * 100:.3f}%')
print(f'Test Accuracy: {test_acc * 100:.3f}%')

knn_classifier_cifar(knn_X_train, y_train, knn_X_train, y_test, seed)

normalized_log_X_train, normalized_log_X_test = log_X_train/255, log_X_test/255
knn_classifier_cifar(normalized_log_X_train, y_train, normalized_log_X_test, y_test, seed)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
def plot_knn_confusion_matrix(X_tr, y_tr, X_te, y_te):
  knn, knn_y_train_pred, knn_y_test_pred = knn_clasifier_cifarX_tr, y_tr, X_te, y_te()
  

  cm = confusion_matrix(y_te,knn_y_test_pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
  disp.plot()

plot_knn_confusion_matrix(knn_X_train,  y_train, knn_X_test, y_test)

"""# Logistic Classifier"""

from sklearn.linear_model import LogisticRegression
log_X_train = X_train.reshape(-1,3072)
log_X_test = X_test.reshape(-1,3072)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

# Hyperparameter tuning
#only l2 supports all different solvers
#'C':[0.01, 0.1, 1, 10, 50, 100]

# saga performed the best on default C,
# parameters = {'penalty': ['l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}
parameters = {'penalty': ['l2'],'C':[0.001, 0.01], 'solver': ['saga']}
sample_size = 100

normalized_log_X_train, normalized_log_X_test = log_X_train/255, log_X_test/255
      
lr = LogisticRegression(random_state=seed, max_iter=350)

gs_lr = GridSearchCV(lr, parameters, cv=3, return_train_score=True)

gs_lr.fit(normalized_log_X_train[:sample_size], y_train[:sample_size])

print("test_scores", gs_lr.cv_results_['mean_test_score'])
print("train_scores", gs_lr.cv_results_['mean_train_score'])

means = gs_lr.cv_results_['mean_test_score']
params = gs_lr.cv_results_['params']
mean_fit_times = gs_lr.cv_results_["mean_fit_time"]
for mean,mean_fit_time,  param in zip(means, mean_fit_times, params):
    print("%f in %f with: %r" % (mean, mean_fit_time,param))

print('Best parameters found:\n',gs_lr.best_params_)



#normalized
lr_clf = LogisticRegression(solver="saga", C = 0.01, max_iter=100, tol=1e-3,  random_state=seed)
normalized_log_X_train, normalized_log_X_test = log_X_train/255, log_X_test/255
lr_clf.fit(normalized_log_X_train,y_train)


print(log_X_train.shape, log_X_test.shape)
lr_y_train_pred = lr_clf.predict(normalized_log_X_train)
lr_y_test_pred = lr_clf.predict(normalized_log_X_test)

train_acc = accuracy_score(y_train, lr_y_train_pred)
test_acc = accuracy_score(y_test, lr_y_test_pred)
print(f'Train Accuracy: {train_acc * 100:.3f}%')
print(f'Test Accuracy: {test_acc * 100:.3f}%')


cm = confusion_matrix(y_test,lr_y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot()

#traning with normalized results, 
lr_clf = LogisticRegression(solver="saga", C = 0.01, max_iter=200, random_state=seed)
normalized_log_X_train, normalized_log_X_test = log_X_train/255, log_X_test/255

print(normalized_log_X_train.shape, normalized_log_X_test.shape)
lr_clf.fit(normalized_log_X_train,y_train)

print(normalized_log_X_train.shape, normalized_log_X_test.shape)
lr_y_train_pred = lr_clf.predict(normalized_log_X_train)
lr_y_test_pred = lr_clf.predict(normalized_log_X_test)

train_acc = accuracy_score(y_train, lr_y_train_pred)
test_acc = accuracy_score(y_test, lr_y_test_pred)
print(f'Train Accuracy: {train_acc * 100:.3f}%')
print(f'Test Accuracy: {test_acc * 100:.3f}%')

lr_clf = LogisticRegression(max_iter=200, random_state=seed)
lr_clf.fit(log_X_train,y_train)

lr_y_train_pred = lr_clf.predict(log_X_train)
lr_y_test_pred = lr_clf.predict(log_X_test)

train_acc = accuracy_score(y_train, lr_y_train_pred)
test_acc = accuracy_score(y_test, lr_y_test_pred)
print(f'Train Accuracy: {train_acc * 100:.3f}%')
print(f'Test Accuracy: {test_acc * 100:.3f}%')

"""## Feedforward Neural Networks"""

from sklearn.neural_network import MLPClassifier
test_size, training_size = 1000, 20000

mlp_X_train = X_train.reshape(-1,3072)
mlp_X_test = X_test.reshape(-1,3072)

parameters = {'hidden_layer_sizes': [(64,64),(64,100,64),(64,64,64)],
                          'alpha':[0.01,0.005,0.001],'learning_rate_init':[0.01,0.001]}
# mlp_clf = MLPClassifier(hidden_layer_sizes=(64,64,64), alpha=0.001, activation='relu',
#                     solver='sgd', n_iter_no_change=100, max_iter=300, learning_rate_init=0.01,
#                     batch_size=256, random_state=seed)
mlp = MLPClassifier(max_iter=200)
clf = GridSearchCV(mlp, parameters, n_jobs=-1, cv=3)
clf.fit(mlp_X_train[:training_size], y_train[:training_size])
print('Best parameters found:\n', clf.best_params_)

# mlp_clf.fit(mlp_X_train[:training_size],y_train[:training_size])
# mlp_y_train_pred = mlp_clf.predict(mlp_X_train[:training_size])
# mlp_y_test_pred = mlp_clf.predict(mlp_X_test[:test_size])

# train_acc = accuracy_score(y_train[:training_size], mlp_y_train_pred[:training_size])
# test_acc = accuracy_score(y_test[:test_size], mlp_y_test_pred[:test_size])
# print(f'Train Accuracy: {train_acc * 100:.3f}%')
# print(f'Test Accuracy: {test_acc * 100:.3f}%')

from sklearn.neural_network import MLPClassifier
test_size, training_size = 1000, 3000

mlp_X_train = X_train.reshape(-1,3072)
mlp_X_test = X_test.reshape(-1,3072)

# parameters = {'hidden_layer_sizes': [(64,64),(64,100,64),(64,64,64)],
#                           'alpha':[0.01,0.001, 0.0001],'learning_rate_init':[0.01,0.001]}
parameters = {'hidden_layer_sizes': [(64,64),(64,100,64),(64,64,64)],
                          'alpha':[0.01,0.001, 0.0001],'learning_rate_init':[0.01,0.001], 'batch_size':[64, 128, 256]}
# mlp_clf = MLPClassifier(hidden_layer_sizes=(64,64,64), alpha=0.001, activation='relu',
#                     solver='sgd', n_iter_no_change=100, max_iter=300, learning_rate_init=0.01,
#                     batch_size=256, random_state=seed)

normalized_mlp_X_train, normalized_mlp_X_test = mlp_X_train/255, mlp_X_test/255

mlp = MLPClassifier(max_iter=200)
clf = GridSearchCV(mlp, parameters, n_jobs=-1, cv=3, return_train_score=True)
clf.fit(normalized_mlp_X_train[:training_size], y_train[:training_size])
print('Best parameters found:\n', clf.best_params_)

# mlp_clf.fit(mlp_X_train[:training_size],y_train[:training_size])
# mlp_y_train_pred = mlp_clf.predict(mlp_X_train[:training_size])
# mlp_y_test_pred = mlp_clf.predict(mlp_X_test[:test_size])

# train_acc = accuracy_score(y_train[:training_size], mlp_y_train_pred[:training_size])
# test_acc = accuracy_score(y_test[:test_size], mlp_y_test_pred[:test_size])
# print(f'Train Accuracy: {train_acc * 100:.3f}%')
# print(f'Test Accuracy: {test_acc * 100:.3f}%')


print("test_scores", clf.cv_results_['mean_test_score'])
print("train_scores", clf.cv_results_['mean_train_score'])

means = clf.cv_results_['mean_test_score']
params = clf.cv_results_['params']
mean_fit_times = clf.cv_results_["mean_fit_time"]
for mean,mean_fit_time,  param in zip(means, mean_fit_times, params):
    print("%f in %f with: %r" % (mean, mean_fit_time,param))

print('Best parameters found:\n',clf.best_params_)

mlp_clf = MLPClassifier(hidden_layer_sizes=(64, 100, 64), alpha=0.0001, activation='relu',
                    solver='sgd', n_iter_no_change=100, max_iter=300, learning_rate_init=0.01,
                    batch_size=256, random_state=seed)

print(mlp_X_train.shape, mlp_X_test.shape)

mlp_clf.fit(mlp_X_train[:1000],y_train[:1000])
mlp_y_train_pred = mlp_clf.predict(mlp_X_train)
mlp_y_test_pred = mlp_clf.predict(mlp_X_test)

train_acc = accuracy_score(y_train, mlp_y_train_pred)
test_acc = accuracy_score(y_test, mlp_y_test_pred)
print(f'Train Accuracy: {train_acc * 100:.3f}%')
print(f'Test Accuracy: {test_acc * 100:.3f}%')

cm = confusion_matrix(y_test,mlp_y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot()

# normalized
mlp_clf = MLPClassifier(hidden_layer_sizes=(64, 100, 64), alpha=0.0001, activation='relu',
                    solver='sgd', n_iter_no_change=100, max_iter=300, learning_rate_init=0.01,
                    batch_size=256, random_state=seed)

print(mlp_X_train.shape, mlp_X_test.shape)
normalized_mlp_X_train, normalized_mlp_X_test = mlp_X_train/255, mlp_X_test/255

mlp_clf.fit(normalized_mlp_X_train,y_train)
mlp_y_train_pred = mlp_clf.predict(normalized_mlp_X_train)
mlp_y_test_pred = mlp_clf.predict(normalized_mlp_X_test)

train_acc = accuracy_score(y_train, mlp_y_train_pred)
test_acc = accuracy_score(y_test, mlp_y_test_pred)
print(f'Train Accuracy: {train_acc * 100:.3f}%')
print(f'Test Accuracy: {test_acc * 100:.3f}%')

cm = confusion_matrix(y_test,mlp_y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot()

# normalized
mlp_clf = MLPClassifier(hidden_layer_sizes=(64, 100, 64), alpha=0.0001, activation='relu',
                    solver='sgd', n_iter_no_change=100, max_iter=300, learning_rate_init=0.001,
                    batch_size=64, random_state=seed)

print(mlp_X_train.shape, mlp_X_test.shape)
normalized_mlp_X_train, normalized_mlp_X_test = mlp_X_train/255, mlp_X_test/255

mlp_clf.fit(normalized_mlp_X_train,y_train)
mlp_y_train_pred = mlp_clf.predict(normalized_mlp_X_train)
mlp_y_test_pred = mlp_clf.predict(normalized_mlp_X_test)

train_acc = accuracy_score(y_train, mlp_y_train_pred)
test_acc = accuracy_score(y_test, mlp_y_test_pred)
print(f'Train Accuracy: {train_acc * 100:.3f}%')
print(f'Test Accuracy: {test_acc * 100:.3f}%')

cm = confusion_matrix(y_test,mlp_y_test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot()

"""# Convolutional Neural Network(CNN)"""

from keras import models, layers, losses
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
(X_train, y_train), (X_test, y_test) = cifar10.load_data() #data is modified above, so reload here
cnn_X_train = X_train.astype('float32')
cnn_X_test = X_test.astype('float32')
cnn_X_train, cnn_X_test = cnn_X_train/255, cnn_X_test/255 #normalize pixel values in range [0,1]
cnn_model = models.Sequential()
cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
cnn_model.add(layers.MaxPooling2D((2, 2)))
cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
cnn_model.add(layers.MaxPooling2D((2, 2)))
cnn_model.add(layers.Conv2D(64, (3, 3), activation='softmax'))
cnn_model.add(layers.Flatten())
cnn_model.add(layers.Dense(64, activation='relu'))
cnn_model.add(layers.Dense(10, activation="softmax"))
cnn_model.compile(optimizer='adam',
              loss= losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model = cnn_model.fit(cnn_X_train, y_train, epochs=13, 
                    validation_data=(cnn_X_test, y_test))

plt.plot(model.history['accuracy'], label='Train accuracy')
plt.plot(model.history['val_accuracy'], label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

test_loss, test_acc = cnn_model.evaluate(cnn_X_test,  y_test, verbose=2)
print(f'Test Accuracy: {test_acc * 100:.3f}%')

#with optimization (not yet)
from keras import models, layers, losses

(X_train, y_train), (X_test, y_test) = cifar10.load_data() #data is modified above, so reload here
cnn_X_train = X_train.astype('float32')
cnn_X_test = X_test.astype('float32')
cnn_X_train, cnn_X_test = cnn_X_train/255, cnn_X_test/255 #normalize pixel values in range [0,1]
cnn_model = models.Sequential()
cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
cnn_model.add(layers.MaxPooling2D((2, 2)))
cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
cnn_model.add(layers.MaxPooling2D((2, 2)))
cnn_model.add(layers.Conv2D(64, (3, 3), activation='softmax'))
cnn_model.add(layers.Flatten())
cnn_model.add(layers.Dense(64, activation='relu'))
cnn_model.add(layers.Dense(10, activation="softmax"))
cnn_model.compile(optimizer=Adam(learning_rate=0.01),
              loss= losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history = cnn_model.fit(cnn_X_train, y_train, epochs=20, 
                    validation_data=(cnn_X_test, y_test), batch_size=64)

plt.plot(history.history['accuracy'], label='Train accuracy')
plt.plot(history.history['val_accuracy'], label='Val accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

test_loss, test_acc = cnn_model.evaluate(cnn_X_test,  y_test, verbose=2)
print(f'Test Accuracy: {test_acc * 100:.3f}%')


cnn_y_test_pred = cnn_model.predict(cnn_X_test)
cm = confusion_matrix(y_test, cnn_y_test_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot()

from keras import models, layers, losses
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
(X_train, y_train), (X_test, y_test) = cifar10.load_data() #data is modified above, so reload here
cnn_X_train = X_train.astype('float32')
cnn_X_test = X_test.astype('float32')
cnn_X_train, cnn_X_test = cnn_X_train/255, cnn_X_test/255 #normalize pixel values in range [0,1]
cnn_model = models.Sequential()
cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
cnn_model.add(layers.MaxPooling2D((2, 2)))
cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
cnn_model.add(layers.MaxPooling2D((2, 2)))
cnn_model.add(layers.Conv2D(64, (3, 3), activation='softmax'))
cnn_model.add(layers.Flatten())
cnn_model.add(layers.Dense(64, activation='relu'))
cnn_model.add(layers.Dense(10, activation="softmax"))
cnn_model.compile(optimizer='adam',
              loss= losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model = cnn_model.fit(cnn_X_train, y_train, epochs=13, 
                    validation_data=(cnn_X_test, y_test))

plt.plot(model.history['accuracy'], label='Train accuracy')
plt.plot(model.history['val_accuracy'], label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

test_loss, test_acc = cnn_model.evaluate(cnn_X_test,  y_test, verbose=2)
print(f'Test Accuracy: {test_acc * 100:.3f}%')

cnn_y_test_pred = cnn_model.predict(cnn_X_test)
cnn_y_test_pred = np.argmax(cnn_y_test_pred, axis=1)

cm = confusion_matrix(y_test, cnn_y_test_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
disp.plot()

from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.optimizers import Adam
from keras import models, layers, losses


(X_train, y_train), (X_test, y_test) = cifar10.load_data() #data is modified above, so reload here
cnn_X_train = X_train.astype('float32')
cnn_X_test = X_test.astype('float32')
cnn_X_train, cnn_X_test = cnn_X_train/255, cnn_X_test/255 #normalize pixel values in range [0,1]

def generate_cnn_model(layerOneSize, layerTwoSize, layerThreeSize, lr):
  cnn_model = models.Sequential()
  cnn_model.add(layers.Conv2D(layerOneSize, (3, 3), activation='relu', input_shape=(32, 32, 3)))
  cnn_model.add(layers.MaxPooling2D((2, 2)))
  cnn_model.add(layers.Conv2D(layerTwoSize, (3, 3), activation='relu'))
  cnn_model.add(layers.MaxPooling2D((2, 2)))
  cnn_model.add(layers.Conv2D(layerThreeSize, (3, 3), activation='softmax'))
  cnn_model.add(layers.Flatten())
  cnn_model.add(layers.Dense(64, activation='relu'))
  cnn_model.add(layers.Dense(10, activation="softmax"))
  cnn_model.compile(optimizer=Adam(learning_rate=lr),
              loss= losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


  return cnn_model

  model = cnn_model.fit(cnn_X_train[:1000], y_train[:1000], epochs=epochs, 
                      validation_split=.2, batch_size=batch_size)



cnn_model = KerasClassifier(build_fn=generate_cnn_model)

lr = [1e-2, 1e-3, 1e-4] #0.01 -  0.00001
batch_size = [32, 64] #16- 64
epochs = [20] #[5, 7, 13, 20] # 5 - 20
layerOneSize=[32, 64, 16]
layerTwoSize=[32, 64, 16]
layerThreeSize=[32, 64, 16]

params = dict(lr=lr, batch_size=batch_size, epochs=epochs, layerOneSize=layerOneSize, layerTwoSize=layerTwoSize, 
              layerThreeSize=layerThreeSize)

cnn_gs = RandomizedSearchCV(cnn_model, params, n_iter=5,  cv=3, return_train_score=True, verbose=1)

cnn_gs.fit(cnn_X_train[:1000], y_train[:1000])

means = cnn_gs.cv_results_['mean_test_score']
params = cnn_gs.cv_results_['params']
mean_fit_times = cnn_gs.cv_results_["mean_fit_time"]
for mean,mean_fit_time,  param in zip(means, mean_fit_times, params):
    print("%f in %f with: %r" % (mean, mean_fit_time,param))

print('Best parameters found:\n',cnn_gs.best_params_)

